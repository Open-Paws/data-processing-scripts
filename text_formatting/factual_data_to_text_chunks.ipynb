{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl9gmfm4nUqn"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q google-cloud-storage google-cloud-aiplatform vertexai\n",
        "!pip install --upgrade google-cloud-aiplatform\n",
        "!pip install --upgrade vertexai\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import auth\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from io import StringIO\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "# Global Configuration\n",
        "CONFIG = {\n",
        "    # Google Cloud Settings\n",
        "    'PROJECT_ID': 'label-studio-424123',\n",
        "    'LOCATION': 'us-central1',\n",
        "\n",
        "    # Storage Buckets\n",
        "    'INPUT_BUCKET': 'accurate-aligned-datasets',\n",
        "    'OUTPUT_BUCKET': 'unranked-training-data',\n",
        "    'OUTPUT_PREFIX': 'unranked-text-chunks/',\n",
        "    'PROGRESS_FOLDER': 'progress-tracking/',\n",
        "\n",
        "    # Model Settings\n",
        "    'MODEL_NAME': 'gemini-1.5-pro',\n",
        "    'TEMPERATURE': 0.3,\n",
        "    'MAX_TOKENS': 4000,\n",
        "    'TOP_P': 0.9,\n",
        "\n",
        "    # File Processing\n",
        "    'VALID_EXTENSIONS': ('.csv', '.json', '.jsonl'),\n",
        "\n",
        "    # Batch Processing\n",
        "    'BATCH_SIZE': 1000,  # Number of rows to process before logging\n",
        "\n",
        "    # Error Handling\n",
        "    'MAX_RETRIES': 3,\n",
        "    'RETRY_DELAY': 1,  # seconds\n",
        "}\n",
        "\n",
        "# Initialize CSV field size limit\n",
        "maxInt = sys.maxsize\n",
        "while True:\n",
        "    try:\n",
        "        csv.field_size_limit(maxInt)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        maxInt = int(maxInt/10)\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Initialize Vertex AI\n",
        "aiplatform.init(project=CONFIG['PROJECT_ID'], location=CONFIG['LOCATION'])\n",
        "\n",
        "class ProgressTracker:\n",
        "    def __init__(self, bucket_name: str, tracker_prefix: str = CONFIG['PROGRESS_FOLDER']):\n",
        "        self.storage_client = storage.Client()\n",
        "        self.bucket = self.storage_client.get_bucket(bucket_name)\n",
        "        self.tracker_prefix = tracker_prefix\n",
        "        # Ensure the tracker prefix ends with a forward slash\n",
        "        if not self.tracker_prefix.endswith('/'):\n",
        "            self.tracker_prefix += '/'\n",
        "        self.tracker_file = f\"{self.tracker_prefix}text_chunk_processing_progress.json\"\n",
        "\n",
        "    def load_progress(self) -> Dict:\n",
        "        \"\"\"Load progress from storage\"\"\"\n",
        "        try:\n",
        "            blob = self.bucket.blob(self.tracker_file)\n",
        "            if blob.exists():\n",
        "                content = blob.download_as_string().decode('utf-8')\n",
        "                return json.loads(content)\n",
        "            else:\n",
        "                print(f\"No existing progress file found at {self.tracker_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading progress: {str(e)}\")\n",
        "        return {'last_processed_file': None, 'last_processed_index': -1}\n",
        "\n",
        "    def save_progress(self, current_file: str, current_index: int):\n",
        "        \"\"\"Save progress to storage\"\"\"\n",
        "        try:\n",
        "            progress = {\n",
        "                'last_processed_file': current_file,\n",
        "                'last_processed_index': current_index,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            blob = self.bucket.blob(self.tracker_file)\n",
        "            blob.upload_from_string(\n",
        "                json.dumps(progress, indent=2),\n",
        "                content_type='application/json'\n",
        "            )\n",
        "            print(f\"Progress saved to {self.tracker_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving progress: {str(e)}\")\n",
        "\n",
        "class DataReformatter:\n",
        "    def __init__(self):\n",
        "        self.storage_client = storage.Client()\n",
        "        vertexai.init(project=CONFIG['PROJECT_ID'], location=CONFIG['LOCATION'])\n",
        "        self.model = GenerativeModel(CONFIG['MODEL_NAME'])\n",
        "        self.progress_tracker = ProgressTracker(\n",
        "            CONFIG['OUTPUT_BUCKET'],\n",
        "            CONFIG['PROGRESS_FOLDER']\n",
        "        )\n",
        "\n",
        "        self.prompt_template = \"\"\"\n",
        "        === CONTEXT ===\n",
        "        {data}\n",
        "\n",
        "        === TASK ===\n",
        "        Analyze and rewrite the above information with these specific goals:\n",
        "\n",
        "        1. ACCURACY:\n",
        "        - Preserve all key facts and details exactly as presented\n",
        "        - Do not add interpretations or assumptions\n",
        "        - Maintain technical precision where present\n",
        "\n",
        "        2. CLARITY:\n",
        "        - Use clear, straightforward language\n",
        "        - Present information in a logical order\n",
        "        - Break complex ideas into digestible parts\n",
        "        - Remove redundancy without losing meaning\n",
        "\n",
        "        3. CONCISENESS:\n",
        "        - Be brief but complete\n",
        "        - Use precise vocabulary\n",
        "        - Eliminate unnecessary words\n",
        "        - Keep sentences focused and direct\n",
        "\n",
        "        4. ENTITY EXTRACTION:\n",
        "        - Groups: Any collective entity including:\n",
        "          * Companies\n",
        "          * Organizations\n",
        "          * Institutions\n",
        "          * Industries\n",
        "          * Categories of people\n",
        "          * Any other collective entity\n",
        "        - Individuals: Specific named people\n",
        "        - Locations: Geographical references\n",
        "        - Concepts: Abstract ideas, terms, metrics\n",
        "        - Species: Any animals or species mentioned\n",
        "        - Events: Specific occurrences or dates\n",
        "\n",
        "        For example, in \"PETA and The Humane Society filed a joint lawsuit against Tyson Foods after whistleblower Jane Smith documented chickens and pigs being abused at their North Carolina facility during the 2023 holiday season\", we would have:\n",
        "        - mentioned_group: [\"PETA\", \"The Humane Society\", \"Tyson Foods\"]\n",
        "        - mentioned_individual: [\"Jane Smith\"]\n",
        "        - mentioned_location: [\"North Carolina\"]\n",
        "        - mentioned_species: [\"chickens\", \"pigs\"]\n",
        "        - mentioned_concept: [\"animal abuse\", \"whistleblowing\"]\n",
        "        - mentioned_events: [\"2023 holiday season lawsuit\"]\n",
        "\n",
        "        Or in \"Local activists from Direct Action Everywhere disrupted a McDonald's board meeting in Chicago to protest their continued use of caged hens in their egg supply chain, while Mercy For Animals released undercover footage of dairy cows at California factory farms\", we would have:\n",
        "        - mentioned_group: [\"Direct Action Everywhere\", \"McDonald's\", \"Mercy For Animals\"]\n",
        "        - mentioned_location: [\"Chicago\", \"California\"]\n",
        "        - mentioned_species: [\"hens\", \"dairy cows\"]\n",
        "        - mentioned_concept: [\"animal confinement\", \"undercover investigation\", \"supply chain\", \"factory farming\"]\n",
        "        - mentioned_events: [\"board meeting disruption\", \"footage release\"]\n",
        "\n",
        "        Your task is to rewrite the information maintaining perfect accuracy while maximizing clarity and conciseness. Extract all relevant entities.\n",
        "\n",
        "        === OUTPUT FORMAT ===\n",
        "        Return a JSON object following this exact schema:\n",
        "        {{\n",
        "            \"summary\": \"Concise 1-2 sentence summary\",\n",
        "            \"main_text\": \"Complete rewritten text preserving all key details\",\n",
        "            \"content_type\": \"article|report|news|blog|social_media|research|other\",\n",
        "            \"source_url\": [],  # List of URLs found in the content. If none found, leave empty. Look for:\n",
        "                # - Web addresses (http://, https://, www.)\n",
        "                # - Social media links\n",
        "                # - Document links\n",
        "                # - Any other referenced URLs or sources\n",
        "                For example:\n",
        "                Input: \"According to a new study published at https://example.com/study123, while Facebook posts at facebook.com/vegangroup show...\"\n",
        "                url: [\"https://example.com/study123\", \"https://facebook.com/vegangroup\"]\n",
        "            \"individual_authors\": [],  # List of individual author names\n",
        "            \"group_authors\": [],  # List of group/organization authors\n",
        "            \"language\": [\"en\"],  # Language codes\n",
        "            \"date\": \"1985-04-12T23:20:50.52Z\",  # Use RFC 3339 timestamp in the date-time format.\n",
        "            \"related_entities\": {{\n",
        "                \"individuals\": [],  # List of mentioned individuals\n",
        "                \"groups\": [],      # List of mentioned groups/organizations\n",
        "                \"species\": [],     # List of mentioned species\n",
        "                \"locations\": [],   # List of mentioned locations\n",
        "                \"events\": []       # List of mentioned events\n",
        "            }},\n",
        "            \"tags\": []  # List of relevant topic tags\n",
        "        }}\n",
        "\n",
        "        Return only the JSON object, no additional text or explanation.\"\"\"\n",
        "\n",
        "    def extract_json_from_response(self, response_text: str) -> str:\n",
        "        \"\"\"Extract JSON from the response text, handling potential ```json prefix\"\"\"\n",
        "        cleaned_text = response_text.strip()\n",
        "        if cleaned_text.startswith('```json'):\n",
        "            cleaned_text = cleaned_text[7:]\n",
        "        if cleaned_text.startswith('```'):\n",
        "            cleaned_text = cleaned_text[3:]\n",
        "        if cleaned_text.endswith('```'):\n",
        "            cleaned_text = cleaned_text[:-3]\n",
        "        return cleaned_text.strip()\n",
        "\n",
        "    def verify_accuracy(self, original_data: str, reformatted_text: str) -> float:\n",
        "        \"\"\"\n",
        "        Verify the accuracy of reformatted text compared to original data.\n",
        "        Returns a score between 0 and 1.\n",
        "        \"\"\"\n",
        "        verification_prompt = \"\"\"\n",
        "        === ORIGINAL TEXT ===\n",
        "        {original}\n",
        "\n",
        "        === REFORMATTED TEXT ===\n",
        "        {reformatted}\n",
        "\n",
        "        === TASK ===\n",
        "        Compare the original text with the reformatted version and score the accuracy of information preservation on a scale of 0 to 1.\n",
        "\n",
        "        Focus on:\n",
        "        1. Factual accuracy - are all key facts preserved exactly?\n",
        "        2. Completeness - is any important information missing?\n",
        "        3. Precision - are technical details maintained accurately?\n",
        "        4. No distortion - is anything misrepresented or changed in meaning?\n",
        "        5. No additions - is there any information added that wasn't in the original?\n",
        "\n",
        "        Return only a single number between 0 and 1, where:\n",
        "\n",
        "        1.0 = Perfect preservation\n",
        "        - Every single fact, detail, and nuance is perfectly preserved\n",
        "        - No information loss whatsoever\n",
        "        - Maintains exact technical precision\n",
        "        - Perfect preservation of context and relationships between facts\n",
        "\n",
        "        0.9 = Near-perfect preservation\n",
        "        - All key facts and important details preserved\n",
        "        - Technical precision maintained\n",
        "\n",
        "        0.8 = Very good preservation\n",
        "        - All core facts and most details preserved\n",
        "        - Technical accuracy maintained for important concepts\n",
        "        - Some very minor details might be simplified, but still accurate\n",
        "\n",
        "        0.7 = Good preservation with minor issues\n",
        "        - All main points preserved\n",
        "        - Some secondary details slightly modified\n",
        "        - Technical precision slightly reduced in a way that isn't entirely accurate\n",
        "        - Context mostly maintained, but simplified in a way that loses some meaningful details\n",
        "\n",
        "        0.6 = Adequate preservation with noticeable gaps\n",
        "        - Core message preserved but some details lost\n",
        "        - Several secondary points missing or modified\n",
        "        - Some technical precision lost\n",
        "        - Context partially simplified\n",
        "\n",
        "        0.5 = Partial preservation with significant issues\n",
        "        - Main points present but some misrepresented\n",
        "        - Important secondary details missing\n",
        "        - Technical precision significantly reduced\n",
        "        - Context partially lost or modified\n",
        "\n",
        "        0.4 = Problematic preservation\n",
        "        - Some main points misrepresented\n",
        "        - Many important details missing\n",
        "        - Technical accuracy compromised\n",
        "        - Context often unclear or modified\n",
        "\n",
        "        0.3 = Poor preservation\n",
        "        - Multiple main points missing or wrong\n",
        "        - Most important details lost\n",
        "        - Technical aspects mostly incorrect\n",
        "        - Context largely lost\n",
        "\n",
        "        0.2 = Very poor preservation\n",
        "        - Most main points missing or incorrect\n",
        "        - Almost all details lost\n",
        "        - Technical aspects wrong\n",
        "        - Context missing or wrong\n",
        "\n",
        "        0.1 = Severe distortion\n",
        "        - Almost all information wrong or missing\n",
        "        - Complete loss of important details\n",
        "        - Technical aspects completely wrong\n",
        "        - Context entirely lost\n",
        "\n",
        "        0.0 = Complete failure\n",
        "        - No accurate information preserved\n",
        "        - Content completely different from original\n",
        "        - No technical accuracy\n",
        "        - Wrong context entirely\n",
        "\n",
        "        Score must be exactly one of these values: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0\n",
        "        Return only the number, no explanation or other text.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            # Check if reformatted_text is a dictionary\n",
        "            if isinstance(reformatted_text, dict):\n",
        "                # Combine summary and main_text from the dictionary\n",
        "                reformatted_content = f\"{reformatted_text.get('summary', '')} {reformatted_text.get('main_text', '')}\"\n",
        "            else:\n",
        "                reformatted_content = reformatted_text\n",
        "\n",
        "            response = self.model.generate_content(\n",
        "                verification_prompt.format(\n",
        "                    original=original_data,\n",
        "                    reformatted=reformatted_content\n",
        "                ),\n",
        "                generation_config={\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"max_output_tokens\": 128,\n",
        "                    \"top_p\": 0.9,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            score_text = response.text.strip()\n",
        "            try:\n",
        "                score = float(score_text)\n",
        "                return max(0.0, min(1.0, score))\n",
        "            except ValueError:\n",
        "                print(f\"Error parsing accuracy score: {score_text}\")\n",
        "                return 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in accuracy verification: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def reformat_data(self, data: str, source_file: str, item_id: int):\n",
        "        max_attempts = CONFIG['MAX_RETRIES']\n",
        "        attempt = 0\n",
        "\n",
        "        while attempt < max_attempts:\n",
        "            try:\n",
        "                prompt = self.prompt_template.format(\n",
        "                    data=data,\n",
        "                    source=source_file,\n",
        "                    item_id=item_id\n",
        "                )\n",
        "\n",
        "                response = self.model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config={\n",
        "                        \"temperature\": CONFIG['TEMPERATURE'],\n",
        "                        \"max_output_tokens\": CONFIG['MAX_TOKENS'],\n",
        "                        \"top_p\": CONFIG['TOP_P'],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    cleaned_response = self.extract_json_from_response(response.text)\n",
        "                    json_response = json.loads(cleaned_response)\n",
        "\n",
        "                    # Verify accuracy but don't include it in the output\n",
        "                    accuracy_score = self.verify_accuracy(data, json_response)\n",
        "                    print(f\"Accuracy score: {accuracy_score}\")\n",
        "\n",
        "                    if accuracy_score >= 0.8:  # Only accept responses with high accuracy\n",
        "                        return json_response  # Return without adding accuracy_score\n",
        "                    else:\n",
        "                        print(f\"Attempt {attempt + 1}: Accuracy score too low ({accuracy_score}), retrying...\")\n",
        "                        attempt += 1\n",
        "                        continue\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSON parsing error: {str(e)}\")\n",
        "                    print(f\"Problematic text: {cleaned_response}\")\n",
        "                    attempt += 1\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reformatting data: {str(e)}\")\n",
        "                attempt += 1\n",
        "                continue\n",
        "\n",
        "        print(\"Max attempts reached\")\n",
        "        return {}\n",
        "\n",
        "    def create_meaningful_filename(self, data: Dict, fact_num: int) -> str:\n",
        "        \"\"\"Create a filename that reflects the content\"\"\"\n",
        "        # Get meaningful elements\n",
        "        groups = data.get('related_entities', {}).get('groups', [])\n",
        "        species = data.get('related_entities', {}).get('species', [])\n",
        "        tags = data.get('tags', [])\n",
        "\n",
        "        elements = []\n",
        "\n",
        "        # Add primary group if exists\n",
        "        if groups:\n",
        "            elements.append(groups[0].replace(' ', '_').lower()[:30])\n",
        "\n",
        "        # Add primary species if exists\n",
        "        if species:\n",
        "            elements.append(species[0].replace(' ', '_').lower()[:30])\n",
        "\n",
        "        # Add primary tag if exists\n",
        "        if tags:\n",
        "            elements.append(tags[0].replace(' ', '_').lower()[:30])\n",
        "\n",
        "        # If no elements found, use first few words of summary\n",
        "        if not elements and data.get('summary'):\n",
        "            first_words = '_'.join(data['summary'].split()[:3]).lower()\n",
        "            elements.append(first_words[:30])\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d')\n",
        "        meaningful_part = '_'.join(elements)\n",
        "        filename = f\"{meaningful_part}_{timestamp}_{fact_num}.json\"\n",
        "        filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '-', '.'])\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def process_bucket(self, input_bucket: str = CONFIG['INPUT_BUCKET'],\n",
        "                      output_bucket: str = CONFIG['OUTPUT_BUCKET'],\n",
        "                      output_prefix: str = CONFIG['OUTPUT_PREFIX']):\n",
        "        \"\"\"\n",
        "        Process all files in the input bucket, transform them, and save to output bucket.\n",
        "        Maintains progress and handles errors gracefully.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\nAccessing storage buckets...\")\n",
        "            output_bucket_obj = self.storage_client.get_bucket(output_bucket)\n",
        "            all_facts = self.load_all_data(input_bucket)\n",
        "\n",
        "            if not all_facts:\n",
        "                print(\"No facts to process\")\n",
        "                return\n",
        "\n",
        "            print(f\"\\nStarting processing of {len(all_facts)} facts...\")\n",
        "\n",
        "            for fact_num, fact in enumerate(all_facts):\n",
        "                try:\n",
        "                    print(f\"\\nProcessing fact {fact_num + 1}/{len(all_facts)}\")\n",
        "                    print(f\"Source file: {fact['source_file']}\")\n",
        "                    print(f\"Index: {fact.get('index', 'N/A')}\")\n",
        "\n",
        "                    # Attempt to reformat the data\n",
        "                    reformatted = self.reformat_data(\n",
        "                        fact['fact_text'],\n",
        "                        fact['source_file'],\n",
        "                        fact_num\n",
        "                    )\n",
        "\n",
        "                    if reformatted:\n",
        "                        # Prepare output data\n",
        "                        output_data = {\n",
        "                            **reformatted,\n",
        "                        }\n",
        "\n",
        "                        try:\n",
        "                            # Create meaningful filename\n",
        "                            filename = self.create_meaningful_filename(reformatted, fact_num)\n",
        "\n",
        "                            # Ensure proper path construction\n",
        "                            output_path = os.path.join(output_prefix, filename).replace('\\\\', '/')\n",
        "\n",
        "                            # Upload to storage\n",
        "                            output_blob = output_bucket_obj.blob(output_path)\n",
        "                            output_blob.upload_from_string(\n",
        "                                json.dumps(reformatted, indent=2),\n",
        "                                content_type='application/json'\n",
        "                            )\n",
        "\n",
        "                            # Update progress\n",
        "                            try:\n",
        "                                self.progress_tracker.save_progress(\n",
        "                                    fact['source_file'],\n",
        "                                    fact.get('index', fact_num)\n",
        "                                )\n",
        "                                print(\"Progress saved successfully\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"Warning: Failed to save progress: {str(e)}\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error saving output file: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                    else:\n",
        "                        print(f\"Warning: Failed to reformat fact {fact_num + 1}\")\n",
        "                        continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing fact {fact_num + 1}: {str(e)}\")\n",
        "                    print(\"Continuing with next fact...\")\n",
        "                    continue\n",
        "\n",
        "            print(\"\\nProcessing complete!\")\n",
        "            print(f\"Processed {len(all_facts)} facts\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical error in process_bucket: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def list_all_blobs(self, bucket: storage.bucket.Bucket) -> List[storage.blob.Blob]:\n",
        "        \"\"\"Recursively list all blobs in bucket including those in subfolders\"\"\"\n",
        "        all_blobs = []\n",
        "\n",
        "        # Get all prefixes (folders) first\n",
        "        prefixes = set()\n",
        "        for blob in bucket.list_blobs():\n",
        "            if '/' in blob.name:\n",
        "                prefix = blob.name.split('/')[0]\n",
        "                prefixes.add(prefix + '/')\n",
        "\n",
        "        print(f\"Found folders: {prefixes}\")\n",
        "\n",
        "        # Process root files\n",
        "        for blob in bucket.list_blobs():\n",
        "            if any(blob.name.lower().endswith(ext) for ext in CONFIG['VALID_EXTENSIONS']):\n",
        "                print(f\"Adding root blob: {blob.name}\")\n",
        "                all_blobs.append(blob)\n",
        "\n",
        "        # Process each prefix (folder)\n",
        "        for prefix in prefixes:\n",
        "            print(f\"\\nProcessing folder: {prefix}\")\n",
        "            folder_blobs = bucket.list_blobs(prefix=prefix)\n",
        "            for blob in folder_blobs:\n",
        "                if any(blob.name.lower().endswith(ext) for ext in CONFIG['VALID_EXTENSIONS']):\n",
        "                    print(f\"Adding blob: {blob.name}\")\n",
        "                    all_blobs.append(blob)\n",
        "\n",
        "        # Sort blobs by name for consistent processing\n",
        "        all_blobs.sort(key=lambda x: x.name)\n",
        "\n",
        "        print(f\"\\nTotal valid files found: {len(all_blobs)}\")\n",
        "        for blob in all_blobs:\n",
        "            print(f\"- {blob.name}\")\n",
        "\n",
        "        return all_blobs\n",
        "\n",
        "    def load_all_data(self, input_bucket: str) -> List[Dict[str, Any]]:\n",
        "        print(\"\\nLoading data from files...\")\n",
        "        all_facts = []\n",
        "        input_bucket_obj = self.storage_client.get_bucket(input_bucket)\n",
        "\n",
        "        # Get progress\n",
        "        progress = self.progress_tracker.load_progress()\n",
        "        last_file = progress['last_processed_file']\n",
        "        last_index = progress['last_processed_index']\n",
        "\n",
        "        # Get all blobs including those in subfolders\n",
        "        blobs = self.list_all_blobs(input_bucket_obj)\n",
        "\n",
        "        # Sort blobs by name for consistent ordering\n",
        "        blobs.sort(key=lambda x: x.name)\n",
        "\n",
        "        # Find where to resume from\n",
        "        start_processing = False if last_file else True\n",
        "\n",
        "        for blob in blobs:\n",
        "            # Skip until we reach the last processed file\n",
        "            if not start_processing and last_file:\n",
        "                if blob.name == last_file:\n",
        "                    start_processing = True\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nLoading {blob.name}\")\n",
        "            try:\n",
        "                content = blob.download_as_string().decode('utf-8')\n",
        "\n",
        "                if blob.name.lower().endswith('.jsonl'):\n",
        "                    for i, line in enumerate(content.splitlines()):\n",
        "                        # Skip until we reach the last processed index for the last file\n",
        "                        if blob.name == last_file and i <= last_index:\n",
        "                            continue\n",
        "\n",
        "                        if line.strip():\n",
        "                            all_facts.append({\n",
        "                                \"source_file\": blob.name,\n",
        "                                \"fact_text\": line.strip(),\n",
        "                                \"index\": i\n",
        "                            })\n",
        "\n",
        "                elif blob.name.lower().endswith('.json'):\n",
        "                    df = pd.DataFrame([json.loads(content)])\n",
        "                    for i, row in df.iterrows():\n",
        "                        if blob.name == last_file and i <= last_index:\n",
        "                            continue\n",
        "\n",
        "                        fact_text = \" \".join(f\"{k}: {v}\" for k, v in row.items() if pd.notna(v))\n",
        "                        if fact_text.strip():\n",
        "                            all_facts.append({\n",
        "                                \"source_file\": blob.name,\n",
        "                                \"fact_text\": fact_text,\n",
        "                                \"index\": i\n",
        "                            })\n",
        "\n",
        "                else:  # CSV files\n",
        "                    df = pd.read_csv(StringIO(content))\n",
        "                    for i, row in df.iterrows():\n",
        "                        if blob.name == last_file and i <= last_index:\n",
        "                            continue\n",
        "\n",
        "                        fact_text = \" \".join(f\"{k}: {v}\" for k, v in row.items() if pd.notna(v))\n",
        "                        if fact_text.strip():\n",
        "                            all_facts.append({\n",
        "                                \"source_file\": blob.name,\n",
        "                                \"fact_text\": fact_text,\n",
        "                                \"index\": i\n",
        "                            })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {blob.name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Total facts loaded: {len(all_facts)}\")\n",
        "        return all_facts\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        reformatter = DataReformatter()\n",
        "        print(\"Starting data reformatting...\")\n",
        "\n",
        "        # Use explicit path with trailing slash\n",
        "        reformatter.process_bucket(\n",
        "            input_bucket=CONFIG['INPUT_BUCKET'],\n",
        "            output_bucket=CONFIG['OUTPUT_BUCKET'],\n",
        "            output_prefix=CONFIG['OUTPUT_PREFIX']\n",
        "        )\n",
        "\n",
        "        print(\"Data reformatting complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}